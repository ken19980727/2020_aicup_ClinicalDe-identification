{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "\n",
    "import tensorflow as tf\n",
    "# from seqeval.metrics import f1_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Sentence#</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>醫師</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>：</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>啊</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>回去</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>還好</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304177</th>\n",
       "      <td>照</td>\n",
       "      <td>O</td>\n",
       "      <td>25572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304178</th>\n",
       "      <td>個</td>\n",
       "      <td>O</td>\n",
       "      <td>25572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304179</th>\n",
       "      <td>x</td>\n",
       "      <td>O</td>\n",
       "      <td>25572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304180</th>\n",
       "      <td>光</td>\n",
       "      <td>O</td>\n",
       "      <td>25572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304181</th>\n",
       "      <td>。</td>\n",
       "      <td>O</td>\n",
       "      <td>25572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304182 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1  Sentence#\n",
       "0       醫師  O          1\n",
       "1        ：  O          1\n",
       "2        啊  O          1\n",
       "3       回去  O          1\n",
       "4       還好  O          1\n",
       "...     .. ..        ...\n",
       "304177   照  O      25572\n",
       "304178   個  O      25572\n",
       "304179   x  O      25572\n",
       "304180   光  O      25572\n",
       "304181   。  O      25572\n",
       "\n",
       "[304182 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = pd.read_csv(\"aidai/AIDEA_Train2_p.txt\", sep='\\t'\n",
    "                   ,encoding = 'utf-8',\n",
    "#                    header = None\n",
    "                  )\n",
    "\n",
    "dev_data = pd.read_csv(\"aidai/AIDEA_Dev2_p.txt\",sep = '\\t',encoding = 'utf-8')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    chunk_end = False\n",
    "    if prev_tag == 'L': chunk_end = True\n",
    "    if prev_tag == 'U': chunk_end = True\n",
    "\n",
    "    if prev_tag == 'B' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'U': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'O': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'U': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'O': chunk_end = True\n",
    "\n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
    "        chunk_end = True\n",
    "\n",
    "    return chunk_end\n",
    "\n",
    "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    chunk_start = False\n",
    "\n",
    "    if tag == 'B': chunk_start = True\n",
    "    if tag == 'U': chunk_start = True\n",
    "\n",
    "    if prev_tag == 'L' and tag == 'L': chunk_start = True\n",
    "    if prev_tag == 'L' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'U' and tag == 'L': chunk_start = True\n",
    "    if prev_tag == 'U' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'L': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'I': chunk_start = True\n",
    "\n",
    "    if tag != 'O' and tag != '.' and prev_type != type_:\n",
    "        chunk_start = True\n",
    "\n",
    "    return chunk_start\n",
    "def get_entities(seq, suffix=False):\n",
    "    if any(isinstance(s, list) for s in seq):\n",
    "        seq = [item for sublist in seq for item in sublist + ['O']]\n",
    "\n",
    "    prev_tag = 'O'\n",
    "    prev_type = ''\n",
    "    begin_offset = 0\n",
    "    chunks = []\n",
    "    for i, chunk in enumerate(seq + ['O']):\n",
    "        if suffix:\n",
    "            tag = chunk[-1]\n",
    "            type_ = chunk.split('-')[0]\n",
    "        else:\n",
    "            tag = chunk[0]\n",
    "            type_ = chunk.split('-')[-1]\n",
    "\n",
    "        if end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            chunks.append((prev_type, begin_offset, i-1))\n",
    "        if start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            begin_offset = i\n",
    "        prev_tag = tag\n",
    "        prev_type = type_\n",
    "\n",
    "    return chunks\n",
    "def f1_score(y_true, y_pred, average='micro', suffix=False):\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "#     print(true_entities)\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "#     print(pred_entities)\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "#     print(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "    r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "    score = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "    return score\n",
    "def classification_report(y_true, y_pred, digits=2, suffix=False):\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "\n",
    "    name_width = 0\n",
    "    d1 = defaultdict(set)\n",
    "    d2 = defaultdict(set)\n",
    "    for e in true_entities:\n",
    "        d1[e[0]].add((e[1], e[2]))\n",
    "        name_width = max(name_width, len(e[0]))\n",
    "    for e in pred_entities:\n",
    "        d2[e[0]].add((e[1], e[2]))\n",
    "\n",
    "    last_line_heading = 'macro avg'\n",
    "    width = max(name_width, len(last_line_heading), digits)\n",
    "\n",
    "    headers = [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "    head_fmt = u'{:>{width}s} ' + u' {:>9}' * len(headers)\n",
    "    report = head_fmt.format(u'', *headers, width=width)\n",
    "    report += u'\\n\\n'\n",
    "\n",
    "    row_fmt = u'{:>{width}s} ' + u' {:>9.{digits}f}' * 3 + u' {:>9}\\n'\n",
    "\n",
    "    ps, rs, f1s, s = [], [], [], []\n",
    "    for type_name, true_entities in d1.items():\n",
    "        pred_entities = d2[type_name]\n",
    "        nb_correct = len(true_entities & pred_entities)\n",
    "        nb_pred = len(pred_entities)\n",
    "        nb_true = len(true_entities)\n",
    "\n",
    "        p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "        r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "\n",
    "        report += row_fmt.format(*[type_name, p, r, f1, nb_true], width=width, digits=digits)\n",
    "\n",
    "        ps.append(p)\n",
    "        rs.append(r)\n",
    "        f1s.append(f1)\n",
    "        s.append(nb_true)\n",
    "\n",
    "    report += u'\\n'\n",
    "\n",
    "    # compute averages\n",
    "    report += row_fmt.format('micro avg',\n",
    "                             precision_score(y_true, y_pred, suffix=suffix),\n",
    "                             recall_score(y_true, y_pred, suffix=suffix),\n",
    "                             f1_score(y_true, y_pred, suffix=suffix),\n",
    "                             np.sum(s),\n",
    "                             width=width, digits=digits)\n",
    "    report += row_fmt.format(last_line_heading,\n",
    "                             np.average(ps, weights=s),\n",
    "                             np.average(rs, weights=s),\n",
    "                             np.average(f1s, weights=s),\n",
    "                             np.sum(s),\n",
    "                             width=width, digits=digits)\n",
    "\n",
    "    return report\n",
    "def precision_score(y_true, y_pred, average='micro', suffix=False):\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "\n",
    "    score = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "\n",
    "    return score\n",
    "def recall_score(y_true, y_pred, average='micro', suffix=False):\n",
    "    true_entities = set(get_entities(y_true, suffix))\n",
    "    pred_entities = set(get_entities(y_pred, suffix))\n",
    "\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    score = nb_correct / nb_true if nb_true > 0 else 0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(data['0'].values))\n",
    "words.append(\"ENDPAD\")\n",
    "words.append(\"unk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(words); n_words\n",
    "tags = list(set(data['1'].values))\n",
    "tags.append(\"ENDPAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s['0'].values.tolist(),\n",
    "                                                     s['1'].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence#\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)\n",
    "dev_getter = SentenceGetter(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = getter.sentences\n",
    "dev_sentences = dev_getter.sentences\n",
    "# len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(sentences,key = len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "idx2word = {i: w for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "idx2tag = {i: t for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "dev_X = []\n",
    "for s in dev_sentences:\n",
    "    b = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            b.append(word2idx[w[0]])\n",
    "        except:\n",
    "            b.append(word2idx['unk'])\n",
    "    dev_X.append(b)\n",
    "\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "\n",
    "dev_y = []\n",
    "for s in dev_sentences:\n",
    "    b = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            b.append(tag2idx[w[1]])\n",
    "        except:\n",
    "            tags.append(w[1])\n",
    "            tag2idx[w[1]] = len(tags)-1\n",
    "            idx2tag[len(tags)-1] = w[1]\n",
    "            b.append(tag2idx[w[1]])\n",
    "    dev_y.append(b)\n",
    "    \n",
    "n_tags = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(dev_X,dev_y):\n",
    "    if len(i) != len(j):\n",
    "        print('asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchcrf import CRF\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self,sentences,labels, word_pad_idx, tag_pad_idx, max_len = 500):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.word_pad_idx = word_pad_idx\n",
    "        self.tag_pad_idx = tag_pad_idx\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.sentences[index],self.labels[index])\n",
    "        \n",
    "    def collate_fn(self, datasets):\n",
    "        sentences = [dataset[0] for dataset in datasets]\n",
    "        labels = [dataset[1] for dataset in datasets]\n",
    "        max_sent = max([len(data) for data in sentences])\n",
    "        max_len = max([min(len(sentence), self.max_len) for sentence in sentences])\n",
    "        pad_sentence = []\n",
    "        pad_label = []\n",
    "        for sentence,label in zip(sentences,labels):\n",
    "            \n",
    "            if len(sentence) > max_len:\n",
    "#                 print('asd')\n",
    "                pad_sentence.append(sentence[:max_len])\n",
    "                pad_label.append(label[:max_len])\n",
    "            else:\n",
    "#                 print('zxc')\n",
    "                pad_sentence.append(sentence+[self.word_pad_idx]*(max_len-len(sentence)))\n",
    "                pad_label.append(label+[self.tag_pad_idx]*(max_len-len(label)))\n",
    "        return torch.LongTensor(pad_sentence), torch.LongTensor(pad_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "bs = 4\n",
    "\n",
    "tr_dataset = NERDataset(X,y,word2idx['ENDPAD'],tag2idx['ENDPAD'])\n",
    "train_dataloader = DataLoader(tr_dataset, batch_size=bs,\n",
    "                              collate_fn=tr_dataset.collate_fn,\n",
    "                             )\n",
    "va_dataset = NERDataset(dev_X,dev_y,word2idx['ENDPAD'],tag2idx['ENDPAD'])\n",
    "valid_dataloader = DataLoader(va_dataset, batch_size=bs,\n",
    "                              collate_fn=va_dataset.collate_fn,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tr_dataset:\n",
    "    a = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    }
   ],
   "source": [
    "all_dataloader = {\n",
    "    'train' : train_dataloader,\n",
    "#     'valid' : valid_dataloader,\n",
    "}\n",
    "for i in all_dataloader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddedRnn(nn.Module):\n",
    "    def __init__(self, vocab, hidden_dim, output_vocab, n_layer,word_pad_idx,tag_pad_idx):\n",
    "        super(EmbeddedRnn, self).__init__()\n",
    "        self.n_layer = n_layer\n",
    "        self.embedding_size = 50\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedded = nn.Embedding(vocab, self.embedding_size , padding_idx  = word_pad_idx)\n",
    "        self.lstm = nn.LSTM(self.embedding_size, hidden_dim, num_layers=n_layer,batch_first = True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(2 * hidden_dim, output_vocab)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.crf = CRF(num_tags=output_vocab, batch_first = True)\n",
    "        self.tag_pad_idx = tag_pad_idx\n",
    "        \n",
    "    def forward(self, x, hidden,y_tag):\n",
    "        embedded = self.embedded(x)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc1(output)\n",
    "#         output = self.softmax(output)\n",
    "        if y_tag is not None:\n",
    "            mask = y_tag != self.tag_pad_idx\n",
    "            crf_out = self.crf.decode(output, mask=mask)\n",
    "            crf_loss = -self.crf(output, tags=y_tag, mask=mask)\n",
    "        else:\n",
    "            crf_out = self.crf.decode(output)\n",
    "            crf_loss = None\n",
    "        return crf_out, crf_loss\n",
    "#         return output, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        hidden = Variable(torch.zeros(2 * self.n_layer, batch_size, self.hidden_dim))\n",
    "        cell = Variable(torch.zeros(2 * self.n_layer, batch_size, self.hidden_dim))\n",
    "        return [hidden, cell]\n",
    "#         return hidden\n",
    "    def init_crf_transitions(self, tag_names, imp_value=-100):\n",
    "#         crf = CRF(num_tags=len(tag_names))\n",
    "        num_tags = len(tag_names)\n",
    "        for i in range(num_tags):\n",
    "            tag_name = tag_names[i]\n",
    "            if tag_name[0] in (\"I\") or tag_name == \"ENDPAD\":\n",
    "                torch.nn.init.constant_(self.crf.start_transitions[i], imp_value)\n",
    "        tag_is = {}\n",
    "        for tag_position in (\"B\", \"I\", \"O\"):\n",
    "            tag_is[tag_position] = [i for i, tag in enumerate(tag_names) if tag[0] == tag_position]\n",
    "        impossible_transitions_position = {\n",
    "            \"O\": \"I\",  \n",
    "        }\n",
    "        for from_tag, to_tag_list in impossible_transitions_position.items():\n",
    "            to_tags = list(to_tag_list)\n",
    "            for from_tag_i in tag_is[from_tag]:\n",
    "                for to_tag in to_tags:\n",
    "                    for to_tag_i in tag_is[to_tag]:\n",
    "                        torch.nn.init.constant_(\n",
    "                            self.crf.transitions[from_tag_i, to_tag_i], imp_value\n",
    "                        )\n",
    "        # init impossible B and I transitions to different entity types\n",
    "        impossible_transitions_tags = {\n",
    "            \"B\": \"I\",\n",
    "            \"I\": \"I\",\n",
    "        }\n",
    "        for from_tag, to_tag_list in impossible_transitions_tags.items():\n",
    "            to_tags = list(to_tag_list)\n",
    "            for from_tag_i in tag_is[from_tag]:\n",
    "                for to_tag in to_tags:\n",
    "                    for to_tag_i in tag_is[to_tag]:\n",
    "                        if tag_names[from_tag_i].split(\"-\")[1] != tag_names[to_tag_i].split(\"-\")[1]:\n",
    "                            torch.nn.init.constant_(\n",
    "                                self.crf.transitions[from_tag_i, to_tag_i], imp_value\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddedRnn(\n",
       "  (embedded): Embedding(7579, 50, padding_idx=7577)\n",
       "  (lstm): LSTM(50, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=512, out_features=27, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (relu): ReLU()\n",
       "  (crf): CRF(num_tags=27)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "word_pad_idx = word2idx['ENDPAD']\n",
    "tag_pad_idx = tag2idx['ENDPAD']\n",
    "model = EmbeddedRnn(n_words, 256, n_tags,2,word_pad_idx,tag_pad_idx)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda\n"
     ]
    }
   ],
   "source": [
    "# for x in all_dataloader['train']:\n",
    "#     print(all_dataloader['train'])\n",
    "#     print(x)\n",
    "model.init_crf_transitions(tags)\n",
    "if use_cuda:\n",
    "    print('use_cuda')\n",
    "    model = model.cuda(0)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-3)\n",
    "\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss : 6.133849587515824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▏                                                                         | 1/10 [04:45<42:47, 285.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_F1: 0.004502251125562782\n",
      "\n",
      "train_loss : 4.118219335109401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▍                                                                 | 2/10 [09:37<38:19, 287.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_F1: 0.23779976521884957\n",
      "\n",
      "train_loss : 2.873372875765868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▌                                                         | 3/10 [14:31<33:44, 289.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_F1: 0.4257369614512471\n",
      "\n",
      "train_loss : 2.361416420950756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████▊                                                 | 4/10 [19:20<28:55, 289.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_F1: 0.4837733773377338\n",
      "\n",
      "train_loss : 2.054316849306624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████                                         | 5/10 [23:59<23:51, 286.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_F1: 0.5370774263904035\n",
      "\n",
      "train_loss : 1.8052574842958482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▏                                | 6/10 [28:35<18:52, 283.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_F1: 0.5733944954128442\n",
      "\n",
      "train_loss : 1.5710236426159079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████▍                        | 7/10 [33:22<14:12, 284.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_F1: 0.5934686198091654\n",
      "\n",
      "train_loss : 1.418414828462368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████████████████████████████████████▌                | 8/10 [38:10<09:30, 285.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_F1: 0.6145734757334055\n",
      "\n",
      "train_loss : 1.3304320290864176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████████████████████████▊        | 9/10 [42:45<04:42, 282.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_F1: 0.624126813541107\n",
      "\n",
      "train_loss : 1.305042970260693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [47:37<00:00, 285.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_F1: 0.6333874458874459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "records = {\n",
    "    'loss':[],\n",
    "    'F1':[],\n",
    "}\n",
    "model.train(True)\n",
    "for epoch in tqdm(range(num_epoch)):\n",
    "    # train_loss,valid_loss = [],[]\n",
    "    all_loss = {\n",
    "        'train': [],\n",
    "        'valid': [],\n",
    "    }\n",
    "    print('')\n",
    "    for loader in all_dataloader:\n",
    "        predictions , true_labels  = [],[]\n",
    "        for x, y in all_dataloader[loader]:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.cuda(0) if use_cuda else x\n",
    "            y = y.cuda(0) if use_cuda else y\n",
    "            hidden = model.initHidden(x.size(0))\n",
    "            if use_cuda:\n",
    "                hidden[0] = hidden[0].cuda(0)\n",
    "                hidden[1] = hidden[1].cuda(0)\n",
    "# #                 hidden = hidden.cuda(0)\n",
    "            output, loss = model(x,hidden,y)\n",
    "            if loader == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            all_loss[loader].append(loss.cpu().item()) \n",
    "#             print(output)\n",
    "#             print(y)\n",
    "            predictions.extend([[idx2tag[j] for j in i] for i in output])\n",
    "#             print(predictions)\n",
    "        \n",
    "            for i in y.detach().cpu().numpy():\n",
    "                _ = []\n",
    "                for j in i:\n",
    "                    if j != tag_pad_idx:\n",
    "                        _.append(idx2tag[j])\n",
    "                true_labels.append(_)\n",
    "                \n",
    "#             print(true_labels)\n",
    "#             break\n",
    "        print(f'{loader}_loss : {np.mean(np.array(all_loss[loader]))}')\n",
    "        f_ = f1_score(true_labels,predictions)\n",
    "        print(f'{loader}_F1: {f_}')\n",
    "        if loader == 'valid':\n",
    "            records['loss'].append(np.mean(np.array(all_loss[loader])))\n",
    "            records['F1'].append(f_)\n",
    "\n",
    "# records = np.array(records)\n",
    "# print(classification_report([valid_tags],[pred_tags]))\n",
    "# plt.plot(np.array(records['loss']), label='valid loss')\n",
    "# plt.plot(np.array(records['F1']), label='valid F1')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predictions).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in output:\n",
    "    for j in i:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions , true_labels , x_  = [],[],[]\n",
    "for x, y in all_dataloader['valid']:\n",
    "    optimizer.zero_grad()\n",
    "    x = x.cuda(0) if use_cuda else x\n",
    "    y = y.cuda(0) if use_cuda else y\n",
    "    hidden = model.initHidden(x.size(0))\n",
    "    if use_cuda:\n",
    "        hidden[0] = hidden[0].cuda(0)\n",
    "        hidden[1] = hidden[1].cuda(0)\n",
    "    output, hidden = model(x, hidden)\n",
    "    x_.extend(x.detach().cpu().numpy())\n",
    "    predictions.extend(np.argmax(output.detach().cpu().numpy(), axis=2))\n",
    "    true_labels.extend(np.argmax(y.detach().cpu().numpy(), axis=2))\n",
    "# pred_tags = [idx2tag[p_i] for p, l , __ in zip(predictions, true_labels,x_) for p_i, l_i ,_i in zip(p, l,__) if idx2word[_i] != \"ENDPAD\"]\n",
    "# valid_tags = [idx2tag[l_i] for l, __ in zip(true_labels,x_) for l_i,_i in zip(l,__) if idx2word[_i] != \"ENDPAD\"]\n",
    "# print(classification_report([valid_tags],[valid_tags]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags = []\n",
    "valid_tags = []\n",
    "for p, l , __ in zip(predictions, true_labels,x_):\n",
    "    _1 = []\n",
    "    _2 = []\n",
    "#     _3 = []\n",
    "    for p_i, l_i ,_i in zip(p, l,__):\n",
    "        if idx2word[_i] != \"ENDPAD\":\n",
    "            _1.append(idx2tag[p_i])\n",
    "            _2.append(idx2tag[l_i])\n",
    "#         _3.append(_i)\n",
    "    pred_tags.append(_1)\n",
    "    valid_tags.append(_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(valid_tags,pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
