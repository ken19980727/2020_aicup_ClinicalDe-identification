{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "# import tensorflow as tf\n",
    "from tqdm import tqdm,trange\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = pd.read_csv(\"train_ex40.txt\", sep='\\s',skip_blank_lines=True,encoding = 'utf-8')\n",
    "\n",
    "dev_data = pd.read_csv(\"sample2.data\",sep = '\\s',encoding = 'utf-8')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.isnan(data['Sentence#']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 1234\n",
    "\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"0\"].values.tolist(),\n",
    "                                                     s[\"1\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence#\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)\n",
    "dev_getter = SentenceGetter(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n",
    "dev_sentences = [[word[0] for word in sentence] for sentence in dev_getter.sentences]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[s[1] for s in sent] for sent in getter.sentences]\n",
    "dev_labels = [[s[1] for s in sent] for sent in dev_getter.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_values = list(set(data[\"1\"].values))\n",
    "tag_values.append(\"PAD\")\n",
    "tag_values\n",
    "# tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "# tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 500\n",
    "bs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0])\n",
    "a = [tokenizer.tokenize(i) for i in sentences[0]]\n",
    "a1 = tokenizer.encode(sentences[0])\n",
    "print(a)\n",
    "print(a1)\n",
    "b = [tokenizer.convert_tokens_to_ids(i) for i in a] \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        if n_subwords > 1 and 'B-' in label:\n",
    "            labels.extend([label])\n",
    "            _ = 'I-' + label.split('B-')[1]\n",
    "            if _ not in tag_values:\n",
    "                tag_values.append(_)\n",
    "                print(_)\n",
    "            labels.extend([_] * (n_subwords-1))\n",
    "        else:\n",
    "            labels.extend([label] * n_subwords)\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_texts_and_labels = [\n",
    "#     tokenize_and_preserve_labels(sent, labs)\n",
    "#     for sent, labs in zip(sentences, labels)\n",
    "# ]\n",
    "# print('done')\n",
    "# dev_tokenized_texts_and_labels = [\n",
    "#     tokenize_and_preserve_labels(sent, labs)\n",
    "#     for sent, labs in zip(dev_sentences, dev_labels)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent_labels(sentence,label):\n",
    "    return tokenizer.encode(sentence),['O']+label+['O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    encode_sent_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences, labels)\n",
    "]\n",
    "print('done')\n",
    "dev_tokenized_texts_and_labels = [\n",
    "    encode_sent_labels(sent, labs)\n",
    "    for sent, labs in zip(dev_sentences, dev_labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_texts_and_labels[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "tokenized_labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "\n",
    "dev_tokenized_texts = [token_label_pair[0] for token_label_pair in dev_tokenized_texts_and_labels]\n",
    "dev_tokenized_labels = [token_label_pair[1] for token_label_pair in dev_tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in zip(dev_tokenized_texts,dev_tokenized_labels):\n",
    "#     if len(i) != len(j):\n",
    "#         print('asd')\n",
    "for idx,i in enumerate(dev_tokenized_texts_and_labels):\n",
    "    if 100 in i:\n",
    "        print(idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "print(dev_tokenized_texts[idx])\n",
    "print(dev_tokenized_labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "#                           maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "#                           truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# dev_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in dev_tokenized_texts],\n",
    "#                           maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "#                           truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids = pad_sequences([txt for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "\n",
    "dev_input_ids = pad_sequences([txt for txt in dev_tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in tokenized_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "dev_tags =pad_sequences([[tag2idx.get(l) for l in lab] for lab in dev_tokenized_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "dev_attention_masks = [[float(i != 0.0) for i in ii] for ii in dev_input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.LongTensor(input_ids)\n",
    "val_inputs = torch.LongTensor(dev_input_ids)\n",
    "tr_tags = torch.LongTensor(tags)\n",
    "val_tags = torch.LongTensor(dev_tags)\n",
    "tr_masks = torch.tensor(attention_masks)\n",
    "val_masks = torch.tensor(dev_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs,val_masks,val_tags)\n",
    "# valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data,batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-chinese\",\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"ckiplab/bert-base-chinese-ner\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=1e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 30\n",
    "max_grad_norm = 2.9\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Store the average loss after each epoch so we can plot them.\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# t_dataloader = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "loss_all = []\n",
    "for _ in trange(4, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    predictions , true_labels = [], []\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "    print(total_loss)\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    if 'PAD' not in pred_tags:\n",
    "        print(\"Training F1-Score: {}\".format(f1_score([valid_tags], [pred_tags])))\n",
    "        print(classification_report([valid_tags],[pred_tags] ))\n",
    "    else:\n",
    "        pass\n",
    "    loss_all.append(avg_train_loss)\n",
    "plt.plot(loss_all , 'r-o', label=\"training loss\")\n",
    "\n",
    "\n",
    "#     model.eval()\n",
    "#     # Reset the validation loss for this epoch.\n",
    "#     eval_loss, eval_accuracy = 0, 0\n",
    "#     predictions , true_labels = [], []\n",
    "#     for batch in valid_dataloader:\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "#         # Telling the model not to compute or store gradients,\n",
    "#         # saving memory and speeding up validation\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(b_input_ids, token_type_ids=None,\n",
    "#                             attention_mask=b_input_mask, labels=b_labels)\n",
    "#         # Move logits and labels to CPU\n",
    "#         logits = outputs[1].detach().cpu().numpy()\n",
    "#         label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "#         # Calculate the accuracy for this batch of test sentences.\n",
    "#         eval_loss += outputs[0].mean().item()\n",
    "#         predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "#         true_labels.extend(label_ids)\n",
    "\n",
    "#     eval_loss = eval_loss / len(valid_dataloader)\n",
    "#     validation_loss_values.append(eval_loss)\n",
    "#     print(\"Validation loss: {}\".format(eval_loss))\n",
    "#     pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "#                                  for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "#     valid_tags = [tag_values[l_i] for l in true_labels\n",
    "#                                   for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "#     print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "#     print()\n",
    "#     print(classification_report(valid_tags,pred_tags ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_all[20:] , 'b-o', label=\"training loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in pred_tags:\n",
    "    if i == 'PAD':\n",
    "        c +=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "predictions , true_labels ,x_list = [], [],[]\n",
    "for batch in train_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "    logits = outputs[1].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    input_ids = b_input_ids.to('cpu').numpy()\n",
    "    eval_loss += outputs[0].mean().item()\n",
    "    x_list.extend(input_ids)\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.extend(label_ids)\n",
    "eval_loss = eval_loss / len(train_dataloader)\n",
    "# validation_loss_values.append(eval_loss)\n",
    "print(\"Validation loss: {}\".format(eval_loss))\n",
    "X = []\n",
    "for x in x_list:\n",
    "    _ = []\n",
    "    for i in x:\n",
    "        if i!= 0:\n",
    "            _.append(tokenizer.convert_ids_to_tokens(int(i)))\n",
    "    X.append(_)\n",
    "pred_tags = []\n",
    "for p, l in zip(predictions, true_labels):\n",
    "    _ = []\n",
    "    for p_i, l_i in zip(p, l):\n",
    "        if tag_values[l_i] != \"PAD\":\n",
    "            _.append(tag_values[p_i])\n",
    "    pred_tags.append(_)    \n",
    "valid_tags = []\n",
    "for l in true_labels:\n",
    "    _ = []\n",
    "    for l_i in l:\n",
    "        if tag_values[l_i] != \"PAD\":\n",
    "            _.append(tag_values[l_i])\n",
    "    valid_tags.append(_)\n",
    "print(classification_report(valid_tags,pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_list\n",
    "c = 0\n",
    "for i in x_list:\n",
    "    if 100 in i:\n",
    "        c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(valid_tags)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 0\n",
    "fn = 0\n",
    "tp = 0 \n",
    "inpec = []\n",
    "res = False\n",
    "for idx,(x,y_pr,y_tr) in enumerate(zip(X,pred_tags,valid_tags)):\n",
    "    for i,j,k in zip(x,y_pr,y_tr):\n",
    "        if 'time' in k and k != j:\n",
    "            fn+=1\n",
    "            res = True\n",
    "            continue\n",
    "    if res:\n",
    "        inpec.append((x,y_pr,y_tr,[idx+1]*len(x)))\n",
    "        res = False\n",
    "#         elif 'time' in j and k != j:\n",
    "#             fp+=1\n",
    "#         elif 'time' in k and k == j:\n",
    "#             tp+=1\n",
    "print(tp,fp,fn)\n",
    "len(inpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./inspect_roberta_3.txt\",\"w+\",encoding=\"utf-8\") as f:\n",
    "    for i in inpec:\n",
    "        f.write('word')\n",
    "        f.write('\\t')\n",
    "        f.write('pred')\n",
    "        f.write('\\t')\n",
    "        f.write('true')\n",
    "        f.write('\\t')\n",
    "        f.write('#')\n",
    "        f.write('\\t')\n",
    "        f.write('\\n')\n",
    "        for j in zip(i[0],i[1],i[2],i[3]):\n",
    "            for q in j:\n",
    "                f.write(str(q))\n",
    "                f.write('\\t')\n",
    "            f.write('\\n')\n",
    "        f.write('\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test_ex40.txt\",sep = '\\s',encoding='utf-8')\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = 0\n",
    "# for i in test_data.index:\n",
    "#     if test_data['0'][i] == '…':\n",
    "#         w+=1\n",
    "#         test_data['0'][i] = 'ˋ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [w for w in s['0'].values.tolist()]\n",
    "        self.grouped = self.data.groupby('Sentence#').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_getter = testGetter(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sentences = [[word for word in sentence] for sentence in test_getter.sentences]\n",
    "test_sentences = []\n",
    "for idx,sentence in enumerate(test_getter.sentences , start = 1):\n",
    "    _ = []\n",
    "    c = 0\n",
    "    for word in sentence:\n",
    "        _.append(word)\n",
    "        if len(sentence)>511:\n",
    "            c += 1\n",
    "        if c >457:\n",
    "            test_sentences.append(_)\n",
    "            _ = []\n",
    "            c = 0\n",
    "    test_sentences.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "sent_b = []\n",
    "for i in test_sentences:\n",
    "    if '燒' in i:\n",
    "        sent_b.append(i)\n",
    "sent_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(max(test_sentences,key = len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = []\n",
    "c = 1\n",
    "for i in test_sentences:\n",
    "    if len(i) > 500:\n",
    "        print(c,len(i))\n",
    "    try:\n",
    "        tokenized_sentence = tokenizer.encode(i)\n",
    "    except:\n",
    "        print(i)\n",
    "    input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "    pred_valid.append(input_ids)\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in pred_valid:\n",
    "    for j in i[0]:\n",
    "        if j.cpu().item() == 100:\n",
    "            c += 1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_label_indices = []\n",
    "c = 0\n",
    "for i in pred_valid:\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            output = model(i)\n",
    "        except:\n",
    "            print(len(i[0]))\n",
    "            print(c)\n",
    "    c += 1\n",
    "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "    all_label_indices.append(label_indices)\n",
    "all_label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_new_tokens = []\n",
    "all_new_labels = []\n",
    "for i in range(len(pred_valid)):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(pred_valid[i].to('cpu').numpy()[0])\n",
    "    new_tokens, new_labels = [], []\n",
    "    for token, label_idx in zip(tokens, all_label_indices[i][0]):\n",
    "        if token.startswith(\"##\"):\n",
    "            print(token)\n",
    "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "        else:\n",
    "            new_labels.append(tag_values[label_idx])\n",
    "            new_tokens.append(token)\n",
    "    all_new_tokens.append(new_tokens)\n",
    "    all_new_labels.extend(new_labels[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(all_new_labels,test_data.index):\n",
    "    test_data['1'][j] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data\n",
    "test_data.to_csv('final_roberta_4.txt',sep='\\t',index=None,encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upload_f(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t,p) for w, t ,p in zip(s['0'].values.tolist(),\n",
    "                                                     s['1'].values.tolist(),\n",
    "                                                     s['2'].values.tolist())]\n",
    "        self.grouped = self.data.groupby('2').apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = Upload_f(test_data)\n",
    "len(getter.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def art_append(art_id,s_id,e_id,text,ner_type):\n",
    "    q = []\n",
    "    q.append(art_id)\n",
    "    q.append(s_id)\n",
    "    q.append(e_id)\n",
    "    q.append(text)\n",
    "    q.append(ner_type)\n",
    "    return q\n",
    "\n",
    "upload = []\n",
    "for sentence in getter.sentences:\n",
    "    str_len = 0\n",
    "    res = False\n",
    "    s = ''\n",
    "    n_t = ''\n",
    "    res2 = False\n",
    "    for sent in sentence:\n",
    "        if not (res or res2) and (sent[1] == 'O' or 'I-' in sent[1]):\n",
    "            str_len += len(sent[0])\n",
    "            continue\n",
    "        elif not res and 'B-' in sent[1]:\n",
    "            res = True\n",
    "            res2 = False\n",
    "            s = sent[0]\n",
    "            n_t = sent[1].split('B-')[1]\n",
    "            st_id = str_len\n",
    "            str_len += len(sent[0])\n",
    "            continue\n",
    "        elif (res or res2) and 'B-' in sent[1]:\n",
    "            res = True\n",
    "            if res2:\n",
    "                res2 = False\n",
    "            end_id = str_len\n",
    "            _1 = art_append(sent[2],st_id,end_id,s,n_t)\n",
    "            upload.append(_1) \n",
    "            s = sent[0]\n",
    "            n_t = sent[1].split('B-')[1]\n",
    "            st_id = str_len\n",
    "            str_len += len(sent[0])\n",
    "            continue\n",
    "        elif res and ('I-' in sent[1]):\n",
    "            res2 = True\n",
    "            s += sent[0]\n",
    "            str_len += len(sent[0])\n",
    "            continue\n",
    "        elif res and (sent[1] == 'O'):\n",
    "            res = False\n",
    "            res2 = False\n",
    "            end_id = str_len\n",
    "            _1 = art_append(sent[2],st_id,end_id,s,n_t)\n",
    "            upload.append(_1) \n",
    "            s = ''\n",
    "            n_t = ''\n",
    "            str_len += len(sent[0])\n",
    "            continue\n",
    "    if res2:\n",
    "        res2 = False\n",
    "        end_id = str_len\n",
    "        _1 = art_append(sent[2],st_id,end_id,s,n_t)\n",
    "        upload.append(_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./final_roberta_3.tsv\",\"w+\",encoding=\"utf-8\") as f: \n",
    "    f.write('article_id')\n",
    "    f.write('\\t')\n",
    "    f.write('start_position')\n",
    "    f.write('\\t')\n",
    "    f.write('end_position')\n",
    "    f.write('\\t')\n",
    "    f.write('entity_text')\n",
    "    f.write('\\t')\n",
    "    f.write('entity_type')\n",
    "    f.write('\\n')\n",
    "    for q in upload:\n",
    "        for j in q[:-1]:\n",
    "            f.write(str(j))\n",
    "            f.write('\\t')\n",
    "        f.write(str(q[-1]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['醫', '師', '：', '因', '為', '你', '之', '前', '打', '針', '，', '假', '如', '效', '果', '有', '效', '有', '時', '候', '2', '0', '1', '1', '3', '月', '4', '號', '就', '見', '效', '了', '。']\n",
    "# a = inpec[2][0]\n",
    "print(a,len(a))\n",
    "b = tokenizer.encode(a)\n",
    "# print(b)\n",
    "test_i = torch.tensor([b]).cuda()\n",
    "# print(test_i)\n",
    "test_o = model(test_i)\n",
    "# print(test_o[0].shape)\n",
    "# print(test_o[0].to('cpu').data.numpy())\n",
    "test_l = np.argmax(test_o[0].to('cpu').data.numpy(), axis=2)\n",
    "# print(test_l)\n",
    "test_tag = [tag_values[i] for i in test_l[0][1:-1]]\n",
    "print(test_tag ,len(test_tag))\n",
    "# tag_values[label_idx]\n",
    "# c = tokenizer.convert_ids_to_tokens(b)\n",
    "# print(c)\n",
    "# tokenizer.convert_tokens_to_ids('[UNK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
